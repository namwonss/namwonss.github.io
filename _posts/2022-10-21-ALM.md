---
layout: single
title: "Augmented Lagrangian amd Method of Multipliers (ALM)"
categories:
 - Algorithm
use_math: true
---
The dual ascent requires conditions to ensure convergence. The limitation of dual ascent is solved by augmented Lagrangian method, also called method of multipliers.

<br/>

This method is to robust dual ascent.
- Convergence without assumptions like strict convexity or finiteness of $f$.

<br/>

Consider the $augmented Lagrangian$,

<br/>

<center>$L(x, y) = f(x) + y^{T}(Ax-b)+\frac{\rho}{2}\left \| Ax-b \right \|^{2}_{2}$</center>

<br/>

where $\rho > 0$ is the penalty parameter.

<br/>

The augmented Lagrangian can be regarded as the un augmented lagrangian associated with the constraints,

<br/>

<center>$\min_{x\in\mathbb{R}^{n}} f(x)+ \frac{\rho}{2} \left \| Ax-b \right \|^{2}_{2}$</center>

<br/>

<center>$s.t. \ Ax=b$</center>

<br/>

Clearly equivalent to original problem. Hence the both equation are same, because for any feasible $x$, the term $\frac{\rho}{2} \left \| Ax-b \right \|^{2}_{2}$ becomes zero.

<br/>

The associated dual function is as follows,

<br/>

<center>$g_{\rho}(y) = l_{\rho}(x,y)$</center>

<br/>

Then, adding term $\frac{\rho}{2}||Ax-b||^{2}_{2}$ to $f\left ( x \right )$ makes $g_{\rho}(y)$ differentiable under rather mild conditions than on the original problem.

<br/>

Applying dual ascent to the modified algorithm is as follows,

<br/>

<center>$x^{k+1}=argmin_{x\in \mathbb{R}^n}\ L\left (x, y^{k}\right )$</center>

<br/>

<center>$y^{k+1}=y^{k}+\rho\left ( Ax^{k+1}-b \right )$</center>.

<br/>

Discussion. Why is the convergence of ALM better than duan ascent?

