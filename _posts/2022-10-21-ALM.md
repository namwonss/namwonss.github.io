---
layout: single
title: "Augmented Lagrangian amd Method of Multipliers (ALM)"
categories:
 - Algorithm
use_math: true
---
The dual ascent requires conditions to ensure convergence. The limitation of dual ascent is solved by augmented Lagrangian method, also called method of multipliers.

<br/>

This method is to robust dual ascent.
- Convergence without assumptions like strict convexity or finiteness of $f$.

<br/>

Consider the *augmented Lagrangian*,
<br/>
<center>$L(x, y) = f(x) + y^{T}(Ax-b)+\frac{\rho}{2}\left \| Ax-b \right \|^{2}_{2}$</center>
<br/>

where &\rho > 0& is the penalty parameter.
<br/>
The augmented Lagrangian can be regarded as the un augmented lagrangian associated with the constraints,
<br/>
<center>$\min_{x\in\mathbb{R}^{n}} f(x)+ \frac{\rho}{2} \left \| Ax-b \right \|^{2}_{2}$</center>
<br/>
<center>$s.t. \ Ax=b$</center>
<br/>
Clearly equivalent to original problem.
